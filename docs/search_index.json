[["index.html", "An introduction to Quantitative Thinking The Nature of Knowledge", " An introduction to Quantitative Thinking Ira Lloyd Parsons, PhD 2025-11-12 The Nature of Knowledge Science is among the most powerful disciplines one can participate in. Fundamentally, science is the pursuit of knowledge, ideally fueled by an overpowering inquisitive desire to know more about the world around you. Hopefully if your reading this you already know that. But what you may not yet appreciate is the role and power of processing data to answer your questions. Data is facts, statistics, and knowledge collected together for reference, analysis, and reasoning or calculation that is collected together to represent a system we have an interest in learning more about. Data is fundamental to the progression of knowledge, and conducting the scientific process. The modern age has brought with it enormous advancements in technology, and massive amounts of data. For us, most data is the product of electrical impulses acting upon sensors which measure some aspect of the system. This may be the inductance of electricity within a scales weigh bars, the vector force along the axis of an accelerometer, the time taken for a GPS signal to reach the receiver from the satellite, or the magnitude of light reflected onto a cameras photo diode. Other data may include counts of things existing within a certain defined space at a defined temporal interval and recorded in a notebook. But even these data are then converted to some electronic format with all possible haste. All data processing should begin with an appreciation for how various sensors function, the capability of the observer, and other limitations. Understanding these will help inform you the specific aspect of the system (animal body-weight, greenness, position, number of animals etc.) the sensor (or human) is measuring. Because we live in a dynamic world, these data points captured independently are only somewhat informative. Real scientific power comes by understanding these measurements relative to other data collected in the same system along some interval. This is the foundation of mathematics, specifically algebra and calculus. However, because we live in a dynamic world, perfectly modeling nature is impossible for all intents and purposes. Thus, variation comes into play and we are tasked with assessing the relative variation we observe in the system and separating it from the affect we are exploring. This is known as probability. Statistics is the merger of theoretical mathematics with applied systems in the real world, and allows us to report results tempered by quantified uncertainty. Simply put: Statistics is the science of Learning from Data. My goal of this coursework and textbook are for students to learn valuable critical thinking and problem solving skills. Students should learn to be confident in their problem solving skills and ability to continue learning. Students should exit the course with the ability to identify and carefully define problems, develop clear objectives, identify methods to solve those problems, clearly support the results they find with rational logic, and clearly communicate what you have done and learned in the context of the current body of knowledge to the greater world. Welcome to Quantitative Thinking. ## Acknowledgments {-} Credit for material from this book are due to multiple people, including Dr. Gordon Carstens who agreed to release a dataset for examples, Dr. Garrett Street for creating the his class Fundamentals of Movement Ecology after which this class is loosely modeled, and to Drs. Mevin Hooten and Thomas Hobbs for creating a workshop in Bayesian Statistics. Ira L. Parsons Rapid City, South Dakota "],["about-the-author.html", "About the Author", " About the Author Ira Parsons (https:iraparsons.com) is an Assistant Professor of Data Systems and Grazing Ecology in the Animal Science Department at the South Dakota State West River Research and Extension Center in Rapid City South Dakota. He has a bachelors from Kansas State University, a masters of Science in Animal Science from Texas A&amp;M University (https://oaktrust.library.tamu.edu/items/3508954e-2365-4faa-87ef-2424e02d3e51). Ira earned a PhD in Forest Resources with a concentration in Wildlife Fisheries and Aquaculature from Mississippi State University with his dissertation on grazing ecology (https://www.proquest.com/docview/2759965228?pq-origsite=gscholar&amp;fromopenview=true&amp;sourcetype=Dissertations%20&amp;%20Theses). His research focuses primarily on applying animal movement ecology and nutrition models to precision livestock technology data, creating integrated information systems to enhance research and management of livestock and range systems. "],["disclaimer.html", "Disclaimer", " Disclaimer This material represents work in progress and is not intended to be published as official material, or represent official views of South Dakota State University. "],["science.html", "Chapter 1 Science 1.1 How you know what you know, and is what you know really so? 1.2 History of scientific inquiry 1.3 Paradigms of Statistical Inference", " Chapter 1 Science 1.1 How you know what you know, and is what you know really so? Science - The systematic study of the structure and behavior of the physical and natural world through observation, experimentation, and the testing of theories against the evidence obtained. Oxford dictionary knowledge or a system of knowledge covering general truths or the operation of general laws especially as obtained and tested through the scientific method. Merriam Webster knowledge from the careful study of the structure and behavior of the physical world, especially by watching, measuring, and doing experiments, and the development of theories to describe the results from these activities. Cambridge dictionary 1.2 History of scientific inquiry 1.3 Paradigms of Statistical Inference "],["applying-the-scientific-method.html", "Chapter 2 Applying the Scientific Method 2.1 Program R 2.2 Using R 2.3 Exploratory Data Analysis and Graphics 2.4 Relationships among covariates 2.5 Deterministic functions 2.6 Stoichasticity 2.7 Homework and Review Questions", " Chapter 2 Applying the Scientific Method 2.1 Program R 2.1.1 Introduction to R Program R is a robust statistical modeling software widely adopted and freely available. R is a full fledged computer language with sophisticated data structures. It is created by statisticians as an interactive environment for data analysis, allowing for both interacting with data in an exploratory maneer while maintaining the critical aspects of reproducibility. These scripts serve as a permanent record for analysis you have performed, allowing for quick re-execution at any moment or the incorporation of new data. If you are patient, you will find the unequaled attractiveness and power of R for both data analysis and visualization. Attractive features of R include: Free and open source Runs on all major platforms, including Windows, MacOS, LINUX Script sharing is easy across platforms Large and active community of users, including plethura of new libraries implementing the latest methods Easy to publish and distribute new methods. 2.1.2 Why not use R? Using R is strongly recommended in this class. It is a powerful tool that can manage some of the most taxing of statistical methods, is free, and many times more capable than software that costs many thousands of dollars per user license. It is used extensively by both academia and industry, including tech giants like Amazon and Google. While it is possible to do any of these things in programs like SAS, JMP, SPSS, or even Excel, they are not free, they can be clunky to use, and you will have to figure it out on your own as I haven’t used those softwares in many years. Thus, I would very much encourage you to give R a shot. 2.2 Using R 2.2.1 Installing R To use R, follow the following steps. Go to Program R’s website (https://www.r-project.org) Select the Download R link, select your prefered CRAN mirror (I typically pick Iowa State, here is a link directly to that mirror https://mirror.las.iastate.edu/CRAN/) Select the version for your machine (Windows, Mac etc.) Open the install engine and follow the steps. Opening R should show you the R console, which looks something like this: This is powerful, but likely not as approachable as you would like. To address this, I strongly recommend using an IDE (Integrated Development Environment) which serves to put all of the best features of a programming languange at the forefront. For R, there are many, includinge VSCode, Jupyter Notebooks, and RStudio. I recommend RStudio, and will be using it throughout the course. 1. Go to the link for RStudio (https://posit.co/download/rstudio-desktop/) and download the appropriate Rstudio for your computer. 2. Run the install engine, which should go find your installation of R. !! You must install R first, then RStudio !! 2.2.2 Navigating R Studio Upon opening R studio, you should see an interface that looks something like this. knitr::include_graphics(&quot;images/rstudio.png&quot;) Figure 2.1: RStudio The left paine shows the R console. The top right pane includes tabs showing the objects, datasets, functions and values loaded in the R environment. Other tabs include showing the history of executed code during the current session, connections to servers, GIT, and other tabs loaded as needed depending on what addons you have. The bottom right pain includes tabs showing files in the current folder for the current R project, plots built from executed code, packages and ability to install other packages, Veiwer for markdown files (such as this one), and presentations. When you start or open an R script, it will add a 4th pane in the the top left where .R and .rmd files can be edited. Note that this is the default arrangement, and your appearance is customization as you see fit. 2.2.3 Installing packages Now that you have R installed, you have access to basic functions and that come with the program. However, the power of R comes from the collaborative nature of the R community, and you will find that there are packages which you use all the time. Here is how to download and install packages using a set of packages that I find extremely useful. These packages are documented and maintained on R’s repository, meaning they have gone through rigourous checks to ensure they work with the current versions of R, and have published documentation for their methods and functions. # install.packages(&quot;data.table&quot;) # installs one package at a time. # or you can install a bunch of packages at one time # Libraries to install packages &lt;- c(&quot;data.table&quot;, &quot;tidyverse&quot;, &quot;MCMCvis&quot;, &quot;ggExtra&quot;, &quot;devtools&quot;)# List of packages to check/install # Function to check and install packages install_if_missing = function(pkg) { #Create function command if (!require(pkg, character.only = TRUE)) { # list of functions to iterate over install.packages(pkg, dependencies = TRUE) # install packages command for each package in list library(pkg, character.only = TRUE) # Load each package in the list using the library function } else { cat(paste(&quot;Package&quot;, pkg, &quot;is already installed.\\n&quot;)) # If package is installed already skip and report } } # Apply the function to each package in the list lapply(packages, install_if_missing) # Run install packages command created above ## Loading required package: data.table ## Package data.table is already installed. ## Loading required package: tidyverse ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::between() masks data.table::between() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::first() masks data.table::first() ## ✖ lubridate::hour() masks data.table::hour() ## ✖ lubridate::isoweek() masks data.table::isoweek() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::last() masks data.table::last() ## ✖ lubridate::mday() masks data.table::mday() ## ✖ lubridate::minute() masks data.table::minute() ## ✖ lubridate::month() masks data.table::month() ## ✖ lubridate::quarter() masks data.table::quarter() ## ✖ lubridate::second() masks data.table::second() ## ✖ purrr::transpose() masks data.table::transpose() ## ✖ lubridate::wday() masks data.table::wday() ## ✖ lubridate::week() masks data.table::week() ## ✖ lubridate::yday() masks data.table::yday() ## ✖ lubridate::year() masks data.table::year() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ## Package tidyverse is already installed. ## Loading required package: MCMCvis ## Package MCMCvis is already installed. ## Loading required package: ggExtra ## Package ggExtra is already installed. ## Loading required package: devtools ## Loading required package: usethis ## Package devtools is already installed. ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL However, the beauty of R is that anyone can publish a package to serve there purposes, encapsulate their methods, or share their ideas. And that is what I have done to facilitate sharing data within this course. Please load the package below to install the example datasets which will be used throughout this course. I may add datasets as we go, and will tell you if you need to update to the most current version of this package. If so, simply rerun the code below in your R console. I built the following package, and it is available on my github in the qthink repository. devtools::install_github(&quot;irap93/qthink&quot;) # install a package from github using hte devtools package ## Using GitHub PAT from the git credential store. ## Skipping install of &#39;qthink&#39; from a github remote, the SHA1 (74bb766d) has not changed since last install. ## Use `force = TRUE` to force installation Now, we need to load our packages. Loading packages is accomplished through the library() command in your r console or rscript. library(qthink) # laod packages one by one using the &quot;library&quot; command library(data.table) library(tidyverse) Or, you can load them all at once like this using a for loop, which cycles through each package, checks if its installed and loaded packages &lt;- c(&quot;data.table&quot;, &#39;qthink&#39;,&#39;tidyverse&#39;)# List of packages to check/install # Load each package, install if missing using a loop for (pkg in packages) { if (!requireNamespace(pkg, quietly = TRUE)) { install.packages(pkg) } library(pkg, character.only = TRUE) } 2.2.4 Loading data into the R environment There are numerous ways to load data into R using a number of different libraries. For this lesson, we will be using the data in qthink library to illustrate how to save data and reload it using base R. # load the heifer dataset from the qthink package data(&quot;heifer&quot;) # Load the heifer data from the qthink package head(heifer) # Look at the heifer data # Save it to your workspace, change out the file path name to reflect your computer write.csv(heifer, file = &#39;/Users/iraparsons/Documents/AS791_FundQuantThink/Data/heifer.csv&#39;) # Save the data to your machine Now go look in your folder to see if a csv file was saved. Open it up and look at it. Now lets reload it into the workspace. You can load csv files you create from excel workbooks to load your personal data into R in the same fashion. heifer1 = read.csv(file = &#39;/Users/iraparsons/Documents/AS791_FundQuantThink/Data/heifer.csv&#39;) # Example of how to read the file back into Did a new data file named heifer1 show up in your R environmnent? If so, you have been sucessfull. Now, lets explore the data a bit. 2.3 Exploratory Data Analysis and Graphics If you know about your data, as in you collected it for your research project, you probably already know what everything represents. But in this case, we need to look at the description of the data. I saved metadata describing this dataset in the package, which we can see by running the following queries. help(heifer) # Look at the help and documentation material for the heifer data help(&quot;bodyweight&quot;) # Look at the help and documentation material for the bodyweight data set. The study description should appear in the lower right pane under the ‘help’ tab. When approaching a data set, it is important to do due diligence and ensure that a complete understanding of the data is achieved before further analysis or publication is undertaken. This can be achieved in a number of ways, which will be enumerated below. As with any analysis, an understanding of the studies experimental design is critical to understanding what values may be expected within the data sheet, and will lend insight into what potential analysis may be conducted. When opening a data set, it is important to have an understanding of what to expect. This includes such things as the study design and description, and the meta-data associated with the data sheets. Meta data, at the very least, should include descriptions of the column names, and indicate the type of data you might expect to be within. It may also include information concerning individual observations, such as animals that died or were pulled from the study, why they might have been pulled, and other factors that should be considered when analyzing the data. File and column names should be kept as simple, yet descriptive, as possible. Use well known naming conventions, and avoid the use of spaces, instead using underscores or dots, though dots should be avoided as they are action symbols in python. You will quickly understand the value of having well conceived naming conventions for things you will type a lot, such as the names of columns and data frames. As we go through the class, you will likely notice some of the naming conventions that I have adopted. Feel free to use and adopt these for you own use. You will undoubtedly find a specific style which better suits your specific fancy and needs, but I think it is important that you do adopt a style for your own sanity. 2.3.1 Summary Statistics We might first want to know some basic information about the dataframe we’ve loaded. How many rows are in the dataframe by running nrow(heifer) Second, it might be important to know what our column names are names(heifer) Second, we might be interested in what animals are in the dataframe, which we can see by running unique(heifer$VID) When approaching a dataset, it is important to understand what we have. Now that we know how the data was collected, we need to consider the data structure. str(heifer) # check the structure of the dataset ## spc_tbl_ [77 × 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ VID : num [1:77] 248 249 276 298 322 346 364 367 453 472 ... ## $ Pen : num [1:77] 8 8 6 6 8 6 5 7 6 5 ... ## $ CreepTrt : chr [1:77] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; ... ## $ WeanTrt : chr [1:77] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ D_42_BW : num [1:77] 261 210 177 198 198 ... ## $ Creep_Gain : num [1:77] 34.5 23.1 29 30.8 34.5 ... ## $ Shipping_Loss : num [1:77] -5.44 -4.54 -3.63 -6.8 -3.63 ... ## $ Day56_InitialBW: num [1:77] 287 223 200 218 233 ... ## $ Day56_ADG : num [1:77] 1.218 0.946 1.147 1.064 1.298 ... ## $ Day56_MMBW : num [1:77] 1.218 0.946 1.147 1.064 1.298 ... ## $ Day56_DMI : num [1:77] 6.97 6.54 6.24 6.29 7.29 ... ## $ Day56_Residual : num [1:77] -0.118 0.266 0.266 -0.149 0.376 ... ## $ D_1_EV : num [1:77] 0.44 0.389 0.365 0.555 0.662 0.572 0.318 0.362 0.562 0.769 ... ## $ AVE_TTB : num [1:77] 36.2 23.3 69.8 84.1 51.5 ... ## $ BVFREQ : num [1:77] 81 91.4 76.6 85.7 59.2 ... ## $ BVDUR : num [1:77] 6451 7143 7471 5156 8871 ... ## $ BVFREQsd : num [1:77] 81 91.4 76.6 85.7 59.2 ... ## $ BVDURsd : num [1:77] 6451 7143 7471 5156 8871 ... ## $ uDMI : num [1:77] 7.21 7.11 6.71 6.73 7.57 ... ## $ sdDMI : num [1:77] 2.9 2.39 2.35 2.18 2.65 ... ## $ cvDMI : num [1:77] 0.403 0.336 0.351 0.323 0.349 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. VID = col_double(), ## .. Pen = col_double(), ## .. CreepTrt = col_character(), ## .. WeanTrt = col_character(), ## .. D_42_BW = col_double(), ## .. Creep_Gain = col_double(), ## .. Shipping_Loss = col_double(), ## .. Day56_InitialBW = col_double(), ## .. Day56_ADG = col_double(), ## .. Day56_MMBW = col_double(), ## .. Day56_DMI = col_double(), ## .. Day56_Residual = col_double(), ## .. D_1_EV = col_double(), ## .. AVE_TTB = col_double(), ## .. BVFREQ = col_double(), ## .. BVDUR = col_double(), ## .. BVFREQsd = col_double(), ## .. BVDURsd = col_double(), ## .. uDMI = col_double(), ## .. sdDMI = col_double(), ## .. cvDMI = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; str(bodyweight) # Check the structure of the dataset ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 78 obs. of 21 variables: ## $ Ref ID : num 1 2 3 4 5 6 7 8 9 10 ... ## $ VID : num 248 249 276 298 322 346 364 367 453 472 ... ## $ EID : num 9.82e+14 9.82e+14 9.82e+14 9.82e+14 9.82e+14 ... ## $ Pen : num 8 8 6 6 8 6 5 7 6 5 ... ## $ BW-42 : num 574 463 389 436 436 416 420 352 429 402 ... ## $ BW-1 : num 650 514 453 504 512 446 510 390 508 485 ... ## $ BW0 : num 638 504 445 489 504 441 479 372 481 474 ... ## $ BW7 : num 636 496 450 487 532 398 481 377 492 469 ... ## $ BW14 : num 670 512 477 522 562 418 516 388 508 486 ... ## $ BW21 : num 716 544 508 530 592 443 548 411 544 522 ... ## $ BW28 : num 698 544 499 530 592 447 552 416 544 520 ... ## $ BW35 : num 732 564 534 566 600 464 584 463 558 552 ... ## $ BW42 : num 732 576 550 590 628 480 590 466 586 556 ... ## $ BW49 : num 768 600 548 578 654 468 600 478 578 560 ... ## $ BW56 : num 786 608 594 624 678 518 628 496 616 584 ... ## $ BW70 : num 806 670 636 650 702 560 676 518 650 628 ... ## $ Shipping Loss: num -12 -10 -8 -15 -8 -5 -31 -18 -27 -11 ... ## $ Creep_Gain : num 76 51 64 68 76 30 90 38 79 83 ... ## $ Pre-Wean_ADG : num 1.81 1.21 1.52 1.62 1.81 ... ## $ CreepTrt : chr &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; ... ## $ WeanTrt : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; This gives us a description of every column in the dataset and what time pf data it is. We also probably want to know what the general space eacch variable occupies, which can be done using the summary command. summary(heifer) # Summary statistics for each variable in the dataset ## VID Pen CreepTrt WeanTrt ## Min. :248.0 Min. :5.000 Length:77 Length:77 ## 1st Qu.:672.0 1st Qu.:6.000 Class :character Class :character ## Median :714.0 Median :7.000 Mode :character Mode :character ## Mean :670.8 Mean :6.519 ## 3rd Qu.:756.0 3rd Qu.:7.000 ## Max. :821.0 Max. :8.000 ## D_42_BW Creep_Gain Shipping_Loss Day56_InitialBW ## Min. :160.0 Min. : 5.897 Min. :-52.6167 Min. :155.5 ## 1st Qu.:179.5 1st Qu.:23.133 1st Qu.:-12.2470 1st Qu.:196.8 ## Median :195.0 Median :30.844 Median : -8.1647 Median :208.4 ## Mean :195.3 Mean :29.949 Mean : -9.2486 Mean :208.5 ## 3rd Qu.:207.3 3rd Qu.:35.834 3rd Qu.: -5.4431 3rd Qu.:220.5 ## Max. :260.9 Max. :84.368 Max. : 0.9072 Max. :287.2 ## Day56_ADG Day56_MMBW Day56_DMI Day56_Residual ## Min. :0.05724 Min. : 0.7139 Min. :3.916 Min. :-0.877272 ## 1st Qu.:0.83159 1st Qu.:38.3352 1st Qu.:5.860 1st Qu.:-0.209799 ## Median :0.96550 Median :47.4249 Median :6.244 Median :-0.088852 ## Mean :0.93066 Mean :37.1065 Mean :6.152 Mean : 0.001859 ## 3rd Qu.:1.11022 3rd Qu.:50.5209 3rd Qu.:6.542 3rd Qu.: 0.247658 ## Max. :1.38238 Max. :57.2651 Max. :7.477 Max. : 1.011334 ## D_1_EV AVE_TTB BVFREQ BVDUR ## Min. :0.0000 Min. : 21.14 Min. : 37.57 Min. : 3053 ## 1st Qu.:0.3890 1st Qu.: 32.82 1st Qu.: 60.80 1st Qu.: 5776 ## Median :0.5440 Median : 48.79 Median : 75.09 Median : 6589 ## Mean :0.5812 Mean : 54.79 Mean : 72.25 Mean : 6915 ## 3rd Qu.:0.6670 3rd Qu.: 68.16 3rd Qu.: 82.77 3rd Qu.: 7650 ## Max. :2.2210 Max. :147.13 Max. :130.86 Max. :13055 ## BVFREQsd BVDURsd uDMI sdDMI ## Min. : 14.78 Min. : 1672 Min. :5.103 Min. :1.947 ## 1st Qu.: 60.80 1st Qu.: 5776 1st Qu.:6.374 1st Qu.:2.326 ## Median : 75.09 Median : 6589 Median :6.718 Median :2.432 ## Mean : 71.65 Mean : 6810 Mean :6.681 Mean :2.496 ## 3rd Qu.: 82.77 3rd Qu.: 7650 3rd Qu.:7.033 3rd Qu.:2.645 ## Max. :130.86 Max. :13055 Max. :7.860 Max. :3.226 ## cvDMI ## Min. :0.2816 ## 1st Qu.:0.3364 ## Median :0.3676 ## Mean :0.3764 ## 3rd Qu.:0.3984 ## Max. :0.6268 summary(bodyweight) ## Ref ID VID EID Pen ## Min. : 1.00 Min. :248.0 Min. :9.82e+14 Min. :5.0 ## 1st Qu.:20.25 1st Qu.:672.5 1st Qu.:9.82e+14 1st Qu.:6.0 ## Median :39.50 Median :713.0 Median :9.82e+14 Median :6.5 ## Mean :39.50 Mean :671.2 Mean :9.82e+14 Mean :6.5 ## 3rd Qu.:58.75 3rd Qu.:755.0 3rd Qu.:9.82e+14 3rd Qu.:7.0 ## Max. :78.00 Max. :821.0 Max. :9.82e+14 Max. :8.0 ## BW-42 BW-1 BW0 BW7 ## Min. :352.0 Min. :380.0 Min. :364.0 Min. :334.0 ## 1st Qu.:395.5 1st Qu.:465.0 1st Qu.:449.0 1st Qu.:439.0 ## Median :428.5 Median :501.0 Median :476.0 Median :466.5 ## Mean :429.3 Mean :495.6 Mean :475.3 Mean :464.9 ## 3rd Qu.:455.8 3rd Qu.:521.5 3rd Qu.:502.0 3rd Qu.:491.8 ## Max. :574.0 Max. :650.0 Max. :638.0 Max. :636.0 ## BW14 BW21 BW28 BW35 ## Min. :345.0 Min. :370.0 Min. :364.0 Min. :380.0 ## 1st Qu.:440.2 1st Qu.:470.2 1st Qu.:473.5 1st Qu.:490.2 ## Median :476.5 Median :507.0 Median :507.0 Median :534.0 ## Mean :478.9 Mean :507.6 Mean :507.2 Mean :532.4 ## 3rd Qu.:511.5 3rd Qu.:543.5 3rd Qu.:544.0 3rd Qu.:567.5 ## Max. :670.0 Max. :716.0 Max. :698.0 Max. :732.0 ## BW42 BW49 BW56 BW70 ## Min. :383.0 Min. :400.0 Min. :398.0 Min. :412.0 ## 1st Qu.:500.8 1st Qu.:521.5 1st Qu.:538.5 1st Qu.:572.5 ## Median :547.0 Median :559.0 Median :585.0 Median :622.0 ## Mean :542.5 Mean :557.3 Mean :579.4 Mean :614.2 ## 3rd Qu.:579.5 3rd Qu.:596.0 3rd Qu.:619.5 3rd Qu.:655.0 ## Max. :732.0 Max. :768.0 Max. :786.0 Max. :806.0 ## Shipping Loss Creep_Gain Pre-Wean_ADG CreepTrt ## Min. :-116.00 Min. : 13.00 Min. :0.3095 Length:78 ## 1st Qu.: -26.75 1st Qu.: 51.00 1st Qu.:1.2143 Class :character ## Median : -18.00 Median : 68.00 Median :1.6190 Mode :character ## Mean : -20.28 Mean : 66.27 Mean :1.5778 ## 3rd Qu.: -12.00 3rd Qu.: 79.75 3rd Qu.:1.8988 ## Max. : 2.00 Max. :186.00 Max. :4.4286 ## WeanTrt ## Length:78 ## Class :character ## Mode :character ## ## ## Now we have an understanding of each variable, if NAs are in it, !! DO NOT PUT PERIODS IN FOR MISSING DATA!! This is not only extremely annoying, it messes with Rs ability to automatically categorize variables as numeric, character, integer, etc. Just leave those cells blank when entering the data. This is a convention that is leftover from programs such as SAS. However, should you get such a datasheet, you can write a replace function to put NAs in the place of the periods, but it is extremely annoying to have to do so. Next, lets graphically look at our data. 2.3.2 Exploratory Graphics Histograms Histograms are extremely useful, as they show the number of animals that fit in each area of the group. hist(heifer$D_42_BW) # histogram of the first bodyweight Or we can use the code to create multiple plots in the same graphic to look at everything at once. Here its easier to use the proportion so everything scales. vars = c(&#39;D_42_BW&#39;,&#39;Creep_Gain&#39;,&#39;Shipping_Loss&#39;,&#39;Day56_InitialBW&#39;,&#39;Day56_ADG&#39;,&#39;Day56_MMBW&#39;,&#39;Day56_DMI&#39;,&#39;Day56_Residual&#39;,&#39;D_1_EV&#39;, &#39;AVE_TTB&#39;,&#39;BVFREQ&#39;,&#39;BVDUR&#39;,&#39;BVFREQsd&#39;,&#39;BVDURsd&#39;,&#39;uDMI&#39;,&#39;sdDMI&#39;,&#39;cvDMI&#39;) # List of variables we want histograms of for (v in vars) { # Set up a for loop to make a histogram of each variable (v) in the list of variables (vars) hist(heifer[[v]], # create a histogram (hist) of each variable in data (heifer) and the variable in the list [[v]] probability = T, # actual counts &quot;F&quot; vs. Proportions &quot;T&quot; main = paste(&quot;Histogram of&quot;,v), # Past the text &quot;Histogram of&quot; with the variable name xlab = v) # Label x axis with variable name lines(density(heifer[[v]])) # Add lines of the density distribution &quot;density(heifer[[v]])&quot; } 2.4 Relationships among covariates “There are no routine statistical questions, only questionable statistical routines.” – Sir David Cox “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” – John Tukey Part of the creative process is understanding relationships among covariates within a dataset. Breaking down the data by some categorical variable is the next step in zooming into our dataset. Box and whisker plots, or their modern cousins violin plots, give a compact visual summary of the datasets distributions and can be broken out by categorical variables. They give a visual representation of 5 key variables Minimum: The smallest value excluding outliers First quartile: the 25th percentile Median (Q2): the middle value of the data Third quartile: the 75th percentile Maximum: The largest value of hte data exlcuding outliers It also shows other information such as: Interquartile range (IQR): spread between Q1 and Q3 Whiskers: Indicate variability outside the middle 50% of the data Outliers: Plotted individual points beyond the whiskers This allows you to evaluate the central tendency, spread, and skewness of the dataset, and is useful for comparing results across groups. boxplot(heifer$D_42_BW ~ heifer$WeanTrt) # plot boxplot with continuous variable &quot;D_42_BW&quot; relative to the categorical variable &quot;WeanTrt&quot; on the x axis ggplot(data = heifer, aes(x=WeanTrt, y = D_42_BW, fill = WeanTrt))+ # initiate ggplot,specify data, and define x and y variables geom_violin(draw_quantiles = T)+ # direct ggplot to create a violin plot # geom_boxplot(fill = &#39;NA&#39;) # Direct ggplot to make a box plot # stat_summary(fun = mean, geom = &#39;crossbar&#39;)+ # use stat_summary to draw a crossbar on the plot geom_point(position = &#39;jitter&#39;)+ # Point points on the graph labs(x = &#39;Weaning Treatment&#39;, # X label y = &#39;Bodyweight Day -42, Kg&#39;) # Y label I personally like violin plots because it shows both the distribution of the data. However, it doesn’t include some of the other parameters like mean and standard error that a box plot does. There are ways of adding those afterwards. But they both serve a purpose and you take your pick. However, as you note this is not a simple preweaning treatment. Heifers were also fed different treatments post weaning, in a 2x2 crossover factorial design. So how do we look at these affects? Well, with a bit more data wrangling, we can. For simplicities sake, we will create a new column including the combination of both the pre and post weaning treatments. heifer$pptrt = paste0(heifer$CreepTrt, &#39;_&#39;, heifer$WeanTrt) # Create a new variable column in data combining pre and post weaning treatments head(heifer) # Look at the first 10 lines of the data ## # A tibble: 6 × 22 ## VID Pen CreepTrt WeanTrt D_42_BW Creep_Gain Shipping_Loss Day56_InitialBW ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 248 8 A A 261. 34.5 -5.44 287. ## 2 249 8 A A 210. 23.1 -4.54 223. ## 3 276 6 B A 177. 29.0 -3.63 200. ## 4 298 6 B A 198. 30.8 -6.80 218. ## 5 322 8 A A 198. 34.5 -3.63 233. ## 6 346 6 B A 189. 13.6 -2.27 185. ## # ℹ 14 more variables: Day56_ADG &lt;dbl&gt;, Day56_MMBW &lt;dbl&gt;, Day56_DMI &lt;dbl&gt;, ## # Day56_Residual &lt;dbl&gt;, D_1_EV &lt;dbl&gt;, AVE_TTB &lt;dbl&gt;, BVFREQ &lt;dbl&gt;, ## # BVDUR &lt;dbl&gt;, BVFREQsd &lt;dbl&gt;, BVDURsd &lt;dbl&gt;, uDMI &lt;dbl&gt;, sdDMI &lt;dbl&gt;, ## # cvDMI &lt;dbl&gt;, pptrt &lt;chr&gt; There. Now you should have a column featuring the combination of the pre and post weaning treatments, separated by an underscore. Lets make new plots comparing all of the variables for each treatment vars = c(&#39;D_42_BW&#39;,&#39;Creep_Gain&#39;,&#39;Shipping_Loss&#39;,&#39;Day56_InitialBW&#39;,&#39;Day56_ADG&#39;,&#39;Day56_MMBW&#39;,&#39;Day56_DMI&#39;,&#39;Day56_Residual&#39;,&#39;D_1_EV&#39;, &#39;AVE_TTB&#39;,&#39;BVFREQ&#39;,&#39;BVDUR&#39;,&#39;BVFREQsd&#39;,&#39;BVDURsd&#39;,&#39;uDMI&#39;,&#39;sdDMI&#39;,&#39;cvDMI&#39;) # List of variables to plot trt_colors = c(&quot;A_A&quot; = &quot;#0C2340&quot;, &quot;B_A&quot; = &quot;#003087&quot;, &quot;A_B&quot; = &#39;#C99700&#39;, &quot;B_B&quot;=&#39;#F1EB9C&#39;) # Make a vector defining colors of each treatment for (v in vars) { # for loop to iterate over each variable v in the list vars boxplot(heifer[[v]]~heifer$pptrt, # Make a boxplot main = paste(&quot;Boxplot of&quot;,v), # title boxplot ylab = v, # label y axis xlab = &#39;Treatment&#39;, # label x axis col = trt_colors[levels(factor(heifer$pptrt))]) # assign colors from list trt_colors } 2.4.1 Scatter plots Scatter plots are another great way of looking for patterns in the data relative to another variable. Lets look at the bodyweight data again. head(bodyweight) # Look at head of bodyweight data ## Ref ID VID EID Pen BW-42 BW-1 BW0 BW7 BW14 BW21 BW28 ## &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: 1 248 9.820004e+14 8 574 650 638 636 670 716 698 ## 2: 2 249 9.820004e+14 8 463 514 504 496 512 544 544 ## 3: 3 276 9.820004e+14 6 389 453 445 450 477 508 499 ## 4: 4 298 9.820004e+14 6 436 504 489 487 522 530 530 ## 5: 5 322 9.820004e+14 8 436 512 504 532 562 592 592 ## 6: 6 346 9.820004e+14 6 416 446 441 398 418 443 447 ## BW35 BW42 BW49 BW56 BW70 Shipping Loss Creep_Gain Pre-Wean_ADG CreepTrt ## &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; ## 1: 732 732 768 786 806 -12 76 1.8095238 A ## 2: 564 576 600 608 670 -10 51 1.2142857 A ## 3: 534 550 548 594 636 -8 64 1.5238095 B ## 4: 566 590 578 624 650 -15 68 1.6190476 B ## 5: 600 628 654 678 702 -8 76 1.8095238 A ## 6: 464 480 468 518 560 -5 30 0.7142857 B ## WeanTrt ## &lt;char&gt; ## 1: A ## 2: A ## 3: A ## 4: A ## 5: A ## 6: A This data is in what we call wide format. We need to convert it to long to get greater detail. names(bodyweight) # Look at names of bodyweight data ## [1] &quot;Ref ID&quot; &quot;VID&quot; &quot;EID&quot; &quot;Pen&quot; ## [5] &quot;BW-42&quot; &quot;BW-1&quot; &quot;BW0&quot; &quot;BW7&quot; ## [9] &quot;BW14&quot; &quot;BW21&quot; &quot;BW28&quot; &quot;BW35&quot; ## [13] &quot;BW42&quot; &quot;BW49&quot; &quot;BW56&quot; &quot;BW70&quot; ## [17] &quot;Shipping Loss&quot; &quot;Creep_Gain&quot; &quot;Pre-Wean_ADG&quot; &quot;CreepTrt&quot; ## [21] &quot;WeanTrt&quot; bw.l = melt(bodyweight, # initiate the melt command in data.table package to reshape data wide to long id.vars = c(&#39;Ref ID&#39;,&#39;VID&#39;,&#39;EID&#39;,&#39;Pen&#39;,&#39;Creep_Gain&#39;, # variables to replicate. These columns will stay in the dataset &#39;Pre-Wean_ADG&#39;,&#39;CreepTrt&#39;,&#39;WeanTrt&#39;), variable.name = &#39;Day&#39;, # Variable to pivot longer around value.name = &#39;BW&#39;) # Variable to make longer bw.l[, Day := lapply( # call the bodyweight data table using the datatable packate, use list apply to iterate down each row of the data table str_extract_all(Day, &quot;-?\\\\d+\\\\.?\\\\d*&quot;), # use the str_extract_all function to parse the Day column at the set points as.numeric # data type to designate the extracted numbers )] Now that we have our data, lets look at the relationsip between time and bodyweight ggplot(data = bw.l, aes(x=as.numeric(Day),y=BW, color = factor(VID)))+ # call ggplot designate data and x and y variabls geom_point() # Make a point scatter plot ## Warning: Removed 78 rows containing missing values or values outside the scale range ## (`geom_point()`). Cool. What do we notice about the pattern, and what can we intuite about that from the data? Lets look at more linear relationship. ggplot(data = bw.l, aes(x=as.numeric(Day),y=BW, color = factor(VID)))+ geom_smooth() # ggsmooth plot. Draws a best fit line according to method specified. defaults to smoothing splines ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; ## Warning: Removed 78 rows containing non-finite outside the scale range ## (`stat_smooth()`). ggplot(data = bw.l, aes(x=as.numeric(Day),y=BW, color = factor(VID)))+ geom_smooth(method = &#39;lm&#39;) # ggsmooth plot. Specified using a linear line. ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 78 rows containing non-finite outside the scale range ## (`stat_smooth()`). Now what do we notice? R is using a function called smoothing splines to minimize the residual distance between the line and each observed weight. We will cover residuals more when we talk about linear models. But lets plot one more plot, this time comparing treatments. ggplot(data = bw.l, aes(x=as.numeric(Day),y=BW, color = paste(CreepTrt,WeanTrt)))+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; ## Warning: Removed 78 rows containing non-finite outside the scale range ## (`stat_smooth()`). ggplot(data = bw.l, aes(x=as.numeric(Day),y=BW, color = paste(CreepTrt,WeanTrt)))+ geom_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 78 rows containing non-finite outside the scale range ## (`stat_smooth()`). What do we notice about these graphs? What might we learn from these that was not apparent in the heifer data sheet? 2.5 Deterministic functions 2.5.1 Why deterministic functions Deterministic functions tie patterns to your data. These may be as simple as phenomenalogical descriptions of the data that describe the pattern as accurately as possible. Or they may be the explicit representation of some biological or ecological theory that we wish to test. Either way, they allow us to move forward from the purely explanatory observations of data explored above. Use functions with meaningfull parameters whenever possible, as this increases both the interpretability of the model. You may define your own functions, or, and this is more likely, apply a mathematical function that someone else has previously defined. Different functions describe different responses and should be selected based upon both the hypothesis you are testing, variables you have collected, and response you are attempting to achieve. Deterministic functions are repeatable, where \\(f(x)\\) always gives the same results for \\(x\\) without randomness or other stochastic functions which allows the function’s rules to completely determine its output. Deterministic functions are by their nature, cause and effect. You will most likely use predefined deterministic functions to develop and test your hypothesis. Below are several examples of deterministic functions that are found and used in the literature. Run the code in your R interface and see if you get the same results. Play with the parameters and see how that effects the shape of the resulting data. Perhaps consider creating other plots that describe the data created by each of these functions. All of statistics is centered around describing the fit of data to a model. That data model may be simple or complex, but it is fundamentally seeking to understand a system or the effect of a treatment or thing First order quadratic model R provides the power to consider unique equations and easily see how they describe the relationships between variables. Understanding what the parameters of various functions mean is critical to allow you to estimate what values may take on when looking at the data. However, there is also value in seeing how to create a controlled system that follows a specified pattern, and then assess how consistently the chosen models explain the data. Consider the linear model, which takes the form of \\(y=\\beta_{o} + \\beta_1x + \\epsilon\\); where \\(y\\) is the dependent variable \\(\\beta_o\\) is the intercept on the Y axis, \\(\\beta_1\\) is the slope of the \\(x\\) parameter along the X axis, and \\(\\epsilon\\) is the error for each ith data point along the regression line. x = seq(1,12, 1) # Independent X parameter placed on the X axis b0 = 10 # Specify the Y intercept b1 = 3 # Specify the B1 coefficient i.e. the slope y = b0 + b1*x # integrateing the linear model par(mfrow = c(1,3)) # used for base graphing functions. Sets up a plot figure with 1 ro and 3 columns for 3 plots hist(x) # Histogram of x hist(y) # Histogram of y { plot(x,y, # x and y variables for scatter plot ylim = c(0,max(y)), # manually set the range of y main = expression(y == beta0 + beta*1*x + epsilon)) # use the &quot;expression&quot; function to write the mathematical function for a linear equation abline(lm(y~x), col = &#39;blue&#39;) # add a line according to the function lm, colored blue } par(mfrow = c(1,1)) # reset graphics parameter to 1x1 i.e. one plot per figure window. summary(lm(y~x)) # return the summary of the linear function. lm function is nested inside the summary function. ## Warning in summary.lm(lm(y ~ x)): essentially perfect fit: summary may be ## unreliable ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.727e-15 -1.897e-15 -5.724e-16 8.545e-16 7.789e-15 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.000e+01 1.937e-15 5.163e+15 &lt;2e-16 *** ## x 3.000e+00 2.632e-16 1.140e+16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.147e-15 on 10 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.299e+32 on 1 and 10 DF, p-value: &lt; 2.2e-16 So here we see a basic linear plot derived from data using a deterministic linear model. Next, we will plot the same data, except adding in an element of expected randomness due to some error associated with measuring the data. First order exponential A first exponential function takes on the characteristics of \\(ae^{bx}\\) where \\(a\\) is the initial scaling factor, \\(e\\) represents euler’s number (-2.718), \\(b\\) represents the rate parameter, where positive represents exponential growth, and negative represents exponential decay, and x represents the independent variable, often time, but may be another variable. Positive exponential a = 1 # alpha of the equation b = 1 # beta of the equation x = seq(0,10,length = 100) # x sequence to integrate over y = a*exp(b*x) # calculate the integral of y for each xi plot.new() # create a new plot { plot(x,y, type = &#39;b&#39;, main = &#39;Exponetial&#39;) # create a scatterplot, specifying hollow points &quot;b&quot; lines(x, y) # add a line between consecutive points } Negative Exponential A negative exponential function takes on the characteristics of \\(ae^{-bx}\\) a = 1 b = 0.5 x = seq(0,10,length = 100) y = a*exp(-b*x) plot.new() { plot(x,y, type = &#39;b&#39;, main = &#39;Negative Exponetial&#39;) lines(x, y) # add lines to the plot connecting the points } Saturating exponential growth function a = 1 b = 0.5 x = seq(0,10,length = 100) y = a*(1-exp(-b*x)) plot.new() { plot(x,y, type = &#39;b&#39;, main = &#39;Saturating Exponential Growth&#39;) lines(x, y) } Ricker Function A Ricker function takes on the characteristics of \\(axe^{bx}\\) or alternatively \\(N_t + 1 = N_t e^{r(1-N_t/K)}\\) and is commonly used in ecology and population modeling. It results in a bell shaped curve where \\(N_t\\) represents the population at time \\(t\\), \\(r\\) is the intrinsic growth rate, and \\(K\\) is the carrying capacity. a = 2 b = 0.5 x = seq(0,10,length = 100) y = a*x*exp(-b*x) plot.new() { plot(x,y, type = &#39;b&#39;, main = &#39;Ricker&#39;) lines(x, y) } What happens as the Ricker function approaches infinity? a = 1 b = 1 x = seq(0,10,length = 100) y = a*x*exp(-b*x) plot.new() { plot(x,y, type = &#39;b&#39;, main = &#39;Exponetial&#39;) lines(x, y) abline(lm(x~y)) } Michaelis-Menton Equation The Michaelis-Menton equation is commonly used in enzyme subjugation or predator-prey relationships, and takes on the form of \\(f(v) = V_max[S] / K_m + [S]\\). Here \\(v\\) indicates the rate velocity, \\(V_max\\) rpresents the maximum reaction rate when the enzyme is saturated, [S] indicates the substrate concentration, and \\(K_m\\) the Michaelis constant - the substrate concentration at which \\(V = V_max / 2\\). a = 1 b = 1 x = seq(0,10,length = 100) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 2.5 5.0 5.0 7.5 10.0 y = (a*x)/(b+x) { plot(x,y, ylim = c(0,1), type = &#39;b&#39;, main = &#39;Michaelis-Menton&#39;) lines(x, y) } Logistic Function The basic logistic function \\(f(x) = L/1+e^{-k(x-x_0)}\\) where \\(L\\) is the carrying capacity or the maximum value the function approaches, \\(k\\) is the growth rate, \\(x_0\\) is the inflection point where growth is the fastest, and \\(x\\) represents the variable along which the equation is integrated, often time. a = 0 # Shifts left to right b = 1 # Controls inflection point x = seq(-10,10,length = 100) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -10 -5 0 0 5 10 y = (exp(a+b*x)/(1+exp(a+b*x))) { plot(x,y, ylim = c(0,1), type = &#39;b&#39;, main = &#39;Logistic Function&#39;) lines(x, y) } 2.6 Stoichasticity Stoichasticity represents the variation or randomness in a system. By definition it is non-deterministic, at least within the outline of our model. Thus the same input can lead to different outcomes. However, the probability of receiving a certain outcome within certain range can be calculated using probability, which is where what we know of as Probability distributions come from. This variability, or noise as it is often refered to, can be used to represent what we might expect to find in real world situations as it introduces noise to our deterministic models. For most scientists, noise is just a nuisance that gets in the way of drawing conclusions from the data. The traditional approach is to just assume all variation is normally distributed, or transform the data until it is to allow us to use traditional statistical methods to draw conclusions from the data. Noise affects ecological data in two different ways. The first is measurement error, or the variability or noise associated with the accuracy and specificity of our measurement technique. Big measurement error makes it difficult to estimate parameters and make inference from our data, as it leads to large confidence intervals and lowers statistical power. Process noise is the natural variability that arises from the system, and isn’t so much error as it is the result of the influence of unmeasured variables in the system. Below we will explore several common probability distributions usefull for statistics. 2.6.1 Probability Distributions Normal The normal or more correctly, gaussian distribution is the most commonly employed distribution. \\(z\\) takes on Continuously distributed quantities that can take on positive or negative values. Sums of things are normally distributed. z = rnorm(1000, mean = 0, sd = 1) par(mfrow = c(1,2)) hist(z, main = &#39;Normal&#39;) plot(density(z), main = &#39;Density normal&#39;) #### Lognormal {-} Continously distributed with positive values. Random variables that have the property that their logs are normally distributed. Thus if \\(z\\) is normally distributed then the \\(exp(z)\\) is lognormally distributed. Products of things are normally distributed. z = rlnorm(1000, meanlog = 0, sdlog = 1) par(mfrow = c(1,2)) hist(z, main = &#39;lognormal&#39;) plot(density(z), main = &#39;Density lognormal&#39;) #### Gamma {-} The time required for a specified number of events to occur in a Poisson process. Any continuous quantity that is nonnegative. z = rgamma(n=1000, shape = 1, rate = 1) par(mfrow = c(1,2)) hist(z, main = &#39;gamma&#39;) plot(density(z), main = &#39;Density gamma&#39;) Uniform The uniform distribution takes on any real number, typically bounded by an upper and a lower boundary where any number in between is equally probable. z = runif(n=1000,min = -1,max = 1) par(mfrow = c(1,2)) hist(z, main = &#39;uniform&#39;) plot(density(z), main = &#39;Density uniform&#39;) Poisson Counts of things taht occur randomly over space and time i.e. the number of birds in a forest stand, the number of fish in a kilometer of river, the number of prey captured per minute. z = rpois(n = 1000, lambda = 10) par(mfrow = c(1,2)) hist(z, main = &#39;Poisson&#39;) plot(density(z), main = &#39;Density poisson&#39;) 2.6.2 Stoichastity in functions Randomness is typically predictable within biological systems, and we can add that to our functions expressed above to more closely approximate what we expect to observe in real life. Below we show an example using a simple linear model. First order quadratic set.seed(1) # Ensures our code returns the same random values each time x = runif(n = 100, min = 0, max = 12) # Independent X parameter placed on the X axis b0 = 10 # Specify the Y intercept b1 = 3 # Specify the B1 coefficient i.e. the slope r = rnorm(length(x), mean = 0, sd = 5) y = b0 + b1*x + r par(mfrow = c(1,5)) hist(x) hist(y) hist(r) plot(x,y, ylim = c(0,max(y)), main = expression(y == beta*0 + beta*1*x + epsilon)) abline(lm(y~x), col = &#39;blue&#39;) m = lm(y~x) hist(residuals(m)) summary(lm(y~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2489 -2.8111 -0.4353 2.6214 12.5830 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.1034 1.0291 8.846 3.85e-14 *** ## x 3.1301 0.1473 21.254 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.705 on 98 degrees of freedom ## Multiple R-squared: 0.8217, Adjusted R-squared: 0.8199 ## F-statistic: 451.7 on 1 and 98 DF, p-value: &lt; 2.2e-16 Notice how this has changed the relationship, and due to the randomness, the intercept is no longer exactly where we placed it? Rather, due to the controlled but intentionally introduced error and random variation in the data, the fit has changed and the intercept, and slope have all been affected. However, they may not be that different. Consider how this relates to our perception of truth, and expectations on the underlying systems has discussed in Module 1. 2.7 Homework and Review Questions What do the box and whiskers in a boxplot represent? (hint: use the help(boxplot) to help answer) What is your favorite between the box and whisker plot? What is something you noticed about the effect of pre and post supplemenation on the heifers? Copy and paste the code into your console, and turn in one plot of each kind covered today. What are some common applications for each of the deterministic functions described above? What are some causes of stoichasticity in your study system? What distributions might they take on? Add randomness to one of the other deterministic functions illustrated above. "],["standard-statistics.html", "Chapter 3 Standard Statistics 3.1 Fisher 3.2 Neyman-Pearson 3.3 Null Hypothesis Testing 3.4 Linear Models 3.5 Demonstrating Equivalence 3.6 Liklihood vs. Probability", " Chapter 3 Standard Statistics This is the section that most of you will find more familiar as we cover concepts typically considered “classical” statistics. Studying complex systems with high levels of natural variability requires utilization of statistics to infer pattern and causation from data. 3.1 Fisher Select an appropriate test Set up a null hypothesis \\(H_o\\) Calculate the theoretical probability of the results under \\(H_o\\) Assess the statistical significance of the results Interpret the statistical significance of the results. The appropriate test After selecting an appropriate test, it is important to define the null hypothesis and select the appropriate tests (Fisher 1932). Set up the null hypothesis \\(H_o\\). The \\(H_o\\) derives naturally from the test selected. An example would be the comparison of two population central values (e.g. \\(M_1 - M_2 = 0\\)) or more comparatively \\(M_1 ≠ 0\\). Some parameters of the distribution are calculated from the sample i.e. variance and degress of freedom The rest of the parameters are estimated according to the theoretical distribution. Calculate theoretical probability of results under the \\(H_o\\) hypothesis Here is an example of the frequency distribution for data operating under the assumption that \\(H_o\\) is true x = rnorm(10000, mean = 0, sd = 1) x.den = density(x) { plot(x.den, main = expression(&quot;Theoretical data frequency distribution under H&quot;[o])) abline(v = mean(x), col = &#39;red&#39;) legend(&quot;topright&quot;, legend = expression(H[o]), col = c(&quot;red&quot;), border = NA, lty = 1, lwd = 2, seg.len = 3, bty = &quot;n&quot;) } Assess significance of results Fisher proposed tests of significance that were based on identifying results with low probability of occuring under the null hypothesis (Perezgonzalez 2015). A research result with a low p-value, i.e. a low probability of occuring under the theoretrical distribution of the null hypothesis, may be taken as evidence against the null hypothesis. (Perezgonzalez 2015). How small of a result may be considered significant is the pervue of the researcher (Fisher 1960; Perezgonzalez 2015) or the reader, which is why reporting test statistics and p-values are important. If the p-value is equal to or smaller than the chosen significance level, the results are taken as evidence against the null hypothesis and deemed significant. Interpretation of the results A significant result is representative of a dual statement. Either a rare result that occurs only with a probability of p, or lower, just happened, or the null hypothesis failed to explain the datum collected by the research. A literal intrepretation of the results is that the null hypothesis did not explain our data, thus we infered other processes. x = rnorm(10000, mean = 0, sd = 1) x.den = density(x) # Compute the 95th percentile q95 = quantile(x, 0.95) # Identify the portion of the density curve above the 95th percentile x.shade = x.den$x[x.den$x &gt;= q95] y.shade = x.den$y[x.den$x &gt;= q95] { plot(x.den, main = expression(&quot;Theoretical data frequency distribution under H&quot;[o])) abline(v = mean(x), col = &#39;red&#39;) # Add shaded polygon for upper 5% polygon(c(q95, x.shade, max(x.shade)), c(0, y.shade, 0), col = &#39;pink&#39;, border = NA) legend(&quot;topright&quot;, legend = c(expression(H[o]), &#39;5% rejection region&#39;), col = c(&quot;red&quot;, &#39;pink&#39;), border = NA, lty = 1, lwd = 2, seg.len = 3, bty = &quot;n&quot;) } 3.2 Neyman-Pearson set.seed(3) d = 0.75 par(mfrow = c(1,2)) p1 = rnorm(1000, mean = 0, sd = 1) p2 = rnorm(1000, mean = d, sd = 1) p1.den = density(p1) p2.den = density(p2) xo = rnorm(100, mean=0, sd=0.5) xa = rnorm(100, mean = d, sd = 0.5) xo.den = density(xo) xa.den = density(xa) { plot(p1.den, col = &#39;blue&#39;, lty = 1, main = &#39;population&#39;) lines(p2.den, col = &#39;red&#39;, lty = 2) abline(v = c(mean(xo), mean(xa)), col = c(&#39;blue&#39;,&#39;red&#39;)) plot(xo.den, col = &#39;blue&#39;, lty = 1, main = &#39;sample&#39;) lines(xa.den, col = &#39;red&#39;, lty = 2) abline(v = c(mean(xo), mean(xa)), col = c(&#39;blue&#39;,&#39;red&#39;)) } 3.3 Null Hypothesis Testing Null Hypothesis Significance Testing has been extensively used to make inference and test hypothesis (Stephens, Buskirk, and Del Rio 2007), but has also experienced extensive criticism due to the logical fallacies associated with the assumptions required to make inference regarding test statistics (Hagen, n.d.; Sedgwick, n.d.; Wu 2018). Whichever side of the fence you fall on, you should be aware of both the arguments for and against the utilization of NHST and by extension p-values in making scientific inference. After extensive reading, I will give you my anedotal opionion as of today (09/17/2025) of the issue. Whatever inferential paradigm you choose to use for your research is warranted providing you understand the underlying assumptions the statistical test makes, and exactly what each test statistic is testing (Hagen, n.d.; Wasserstein and Lazar 2016). The most common critique of NHST that I have observed is that scientists and statisticians are not trained how to use tests appropriately, resulting in errant results being reported (Hagen, n.d.; Ioannidis 2005). The magnitude of statistical tests designed for numerous experimental frameworks lend to the problem. Scientists without formal training in the choice of statistical tests are often highly motivated to find significant results, that is, to reject som \\(H_o\\) hypothesis in support of the \\(H_A\\). This motivation is counter to the primary motivation of NHST, which is to reject the \\(H_o\\) with trepidation (Perezgonzalez 2015). This encourages researchers to go in search of significant results by utilizing any number of seemingly plausible available methods to obtain results which we find either either interesting, plausible, personally gratifying, or worse, important for the success of our career. However, whether made through ignorance or malice, that fact that we may report erroneous research results, and that most research results which we may build our research careers on are false (Ioannidis 2005), should give us at least momentary pause. Due to the magnitude of potential scientific tests, which fill tomes of literature and giant volumes of books, teaching standardized statistics which seek to make inference of population central values and affirm or nullify \\(H_o\\) is extremely difficult task to undertake in the limited time we have here. Add in the conflation of posterior and apriori tests, and the conflation of the two, along with nonparametric and parametric tests, creating a clear picture of statistical analysis becomes difficult. So what should we do? I believe the answer rests in the application of two first principles. The first is to hold yourself to the highest set of moral ideals, seeking to reinforce your research findings through severe critique of the results by testing their endurance through multiple tests and alignment with quality research (Mayo 2018). Secondly and of equal importance is to put in serious effort to read and understand the machinery of statistical tests and their underlying assumptions to ensure that the hypothesis you’re testing is the inference they are making (Wu 2018). 3.3.1 Population central values Making inference about a population is the foundation of beginning a good statistical analysis, and often starts in the exploratory analysis we began to explore in chapter 2. A population takes on numerical descriptive values known as parameters. Typical parameters of a population is the mean (\\(u\\)), median (M), standard deviation (\\(σ\\)), and proportion (\\(π\\)) (Ott and Longnecker 2016). For the purposes of hypothesis testing, we will make inference of these parameters in one of two ways. First is to estimate the value of the population, or we will test a hypothesis. The 95% confidence interval is the calculated interval over which 95% of the data will fall given a normal distribution around the central value, \\(u\\) in this case. The 95% confidence interval can be retrieved using the quantile function in R, or calculated using the equation of \\(u ± 1.96σ/√n\\). Here is an example of creating a population with known values of: N 10000 population size mean of 0 standard deviation of 1 n = 10000 # Population size u = 0 # population mean or central value sd = 1 # Standard deviation x = rnorm(100000, mean = u, sd = sd) x.den = density(x) # 95% confidence interval time q0.025 = quantile(x, 0.025) q0.975 = quantile(x, 0.975) # Identify x-values within the 95% CI x.ci &lt;- x.den$x[x.den$x &gt;= q0.025 &amp; x.den$x &lt;= q0.975] y.ci &lt;- x.den$y[x.den$x &gt;= q0.025 &amp; x.den$x &lt;= q0.975] # Identify the portion of the density curve above the 97.5th percentile x.upper = x.den$x[x.den$x &gt;= q0.975] y.upper = x.den$y[x.den$x &gt;= q0.975] # Identify the portion of the density curve below the 0.025th percentile x.lower = x.den$x[x.den$x &lt;= q0.025] y.lower = x.den$y[x.den$x &lt;= q0.025] { plot(x.den, main = &#39;Distribution and rejection regions for normal distribution&#39;) # Add shaded polygon for 95% CI polygon(c(x.ci[1], x.ci, x.ci[length(x.ci)]), c(0, y.ci, 0), col = &#39;lightblue&#39;, border = NA) polygon(c(x.upper[1], x.upper, x.upper[length(x.upper)]), c(0, y.upper, 0), col = &#39;pink&#39;, border = NA) polygon(c(x.lower[1], x.lower, x.lower[length(x.lower)]), c(0, y.lower, 0), col = &#39;pink&#39;, border = NA) abline(v = c(mean(x)-sd(x),mean(x), mean(x)+sd(x)), col = c(&#39;blue&#39;,&#39;red&#39;,&#39;blue&#39;)) legend(&quot;topright&quot;, legend = c(&#39;mean&#39;,&#39;standard deviation&#39;, &#39;95% CI&#39;,&#39;Critical regions&#39;), col = c(&quot;red&quot;,&#39;blue&#39;,NA), fill = c(NA,NA,&#39;lightblue&#39;,&#39;pink&#39;), border = NA, lty = c(1,1,NA,NA), lwd = c(2,2,NA,NA), seg.len = 3, bty = &quot;n&quot;) } Now lets do that in one of our example datasets. library(qthink) data(&quot;bodyweight&quot;) x = bodyweight$BW0 x.den = density(x) # 95% confidence interval time q0.025 = quantile(x, 0.025) q0.975 = quantile(x, 0.975) # Identify x-values within the 95% CI x.ci &lt;- x.den$x[x.den$x &gt;= q0.025 &amp; x.den$x &lt;= q0.975] y.ci &lt;- x.den$y[x.den$x &gt;= q0.025 &amp; x.den$x &lt;= q0.975] # Identify the portion of the density curve above the 97.5th percentile x.upper = x.den$x[x.den$x &gt;= q0.975] y.upper = x.den$y[x.den$x &gt;= q0.975] # Identify the portion of the density curve below the 0.025th percentile x.lower = x.den$x[x.den$x &lt;= q0.025] y.lower = x.den$y[x.den$x &lt;= q0.025] par(mfrow = c(1,2)) { hist(x) plot(x.den) # Add shaded polygon for 95% CI polygon(c(x.ci[1], x.ci, x.ci[length(x.ci)]), c(0, y.ci, 0), col = &#39;lightblue&#39;, border = NA) polygon(c(x.upper[1], x.upper, x.upper[length(x.upper)]), c(0, y.upper, 0), col = &#39;pink&#39;, border = NA) polygon(c(x.lower[1], x.lower, x.lower[length(x.lower)]), c(0, y.lower, 0), col = &#39;pink&#39;, border = NA) abline(v = c(mean(x)-sd(x),mean(x), mean(x)+sd(x)), col = c(&#39;blue&#39;,&#39;red&#39;,&#39;blue&#39;)) # legend(&quot;topright&quot;, # legend = c(&#39;mean&#39;,&#39;standard deviation&#39;, &#39;95% CI&#39;), # col = c(&quot;red&quot;,&#39;blue&#39;,NA), # fill = c(NA,NA,&#39;lightblue&#39;), # border = NA, # lty = c(1,1,NA), # lwd = c(2,2,NA), # seg.len = 1, # bty = &quot;n&quot;) } Here we see a the same values of our population of bodyweights on day 0 from our heifers. Note how they do not follow the neat normal distribution, but do approximate it closely enough, at least visually. Here arises one of the first critiques of NHST that all tests assume that the population distribution follows a normal bell curve if enough samples are taken. Statistical test for u Lets suppose that we have another sample of heifers, perhaps the next years. (keep in mind that at this point we are not considering any treatment options, the only comparison is a made up central value, mean, compared to the current central mean of the current population). See below for the code if you’re not following. n = length(unique(bodyweight$VID)) uo = mean(bodyweight$BW0) # Mean from previous measurements u = 490 # Our new made up mean sd = sd(bodyweight$BW0) # our estimation of bodyweight based upon previous data We choose a critical value of 0.05 to serve as our indicator of significance, i.e. a p value of &lt;= 0.05 is considered significant. This translates to a any value greater than 1.96 standard deviations from the \\(H_o\\) mean is considered significant. This can be mathematically calculated using the equation \\(z = (\\bar{y} - u_o)/ (\\sigma / \\sqrt n)\\) and takes on the following calculation in code. z = (u - uo)/(sd/sqrt(n)) z ## [1] 2.911734 So here we see that our new (made up) test statistic is 2.9117336 above the central value of the population of heifers on day zero in the current dataset. This can be graphically described, demonstrated as follows. xo = rnorm(n = n, mean = uo, sd = sd) xo.den = density(xo) # Critical quantile q95 = 1.96*(sd/sqrt(n)) + uo # Identify x-values within the 95% CI x.ci = xo.den$x[xo.den$x &lt;= q95] y.ci = xo.den$y[xo.den$x &lt;= q95] # Right tail: x &gt; q975 x.upper = xo.den$x[xo.den$x &gt;= q95] y.upper = xo.den$y[xo.den$x &gt;= q95] { plot(xo.den) # Add shaded polygon for 95% CI polygon(c(x.ci[1], x.ci, x.ci[length(x.ci)]), c(0, y.ci, 0), col = &#39;lightblue&#39;, border = NA) # Shade right rejection region polygon(c(x.upper[1], x.upper, x.upper[length(x.upper)]), c(0, y.upper, 0), col = &#39;salmon&#39;, border = NA) abline(v = c(uo, u), col = c(&#39;black&#39;, &#39;blue&#39;), lty = c(1,2), lwd = c(2,2)) legend(&quot;topright&quot;, legend = c(expression(&#39;H&#39;[o]), expression(&#39;H&#39;[a])), col = c(&quot;black&quot;,&#39;blue&#39;), border = NA, lty = c(1,2), lwd = c(2,2), seg.len = 3, bty = &quot;n&quot;) } Here we calculated the rejection region for the distribution constructed under \\(H_o\\) and then observed where our new mean fell. Since it fell in the rejection region, we can assume that it was not constructed under the same theoretical process used to construct the \\(H_o\\) distribution, and thus we can reject it. However, those processes may look similar, and we can see that by sampling observations from each of the \\(H_o\\) and \\(H_a\\) theoretical processes we just compared above and plotting them graphically. set.seed(1) x = rnorm(100, mean = uo, sd = sd) xo = rnorm(100, mean = u, sd = sd) t.test(x) ## ## One Sample t-test ## ## data: x ## t = 119.85, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 472.2023 488.1008 ## sample estimates: ## mean of x ## 480.1516 x.den = density(x) x0.den = density(xo) f = ecdf(xo) f(0) ## [1] 0 { plot(x0.den, main = &#39;&#39;, col = &#39;lightblue&#39;) lines(x.den, col = &#39;red&#39;) abline(v = c(uo,u), col = c(&#39;lightblue&#39;,&#39;red&#39;), lty = 1) legend(&quot;topright&quot;, legend = c(expression(H[0]), expression(H[a])), col = c(&#39;lightblue&#39;,&#39;red&#39;), lty = 1, lwd = 2, bty = &quot;n&quot;) } 3.3.1.1 Two population inference A statistical test is based upon the concept of the following five parts (Ott and Longnecker 2016): Resaerch hypothesis \\(H_a\\) Null hypothesis \\(H_o\\) Test statistic Rejection region Check assumptions, draw conclusions # Sample data xo = rnorm(100, mean = 320, sd = 10) xa = rnorm(100, mean = 350, sd = 10) t.test(xo, xa, var.equal = T) ## ## Two Sample t-test ## ## data: xo and xa ## t = -21.09, df = 198, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -33.04493 -27.39363 ## sample estimates: ## mean of x mean of y ## 320.2967 350.5160 # Compute density xo.den = density(xo) xa.den = density(xa) { # Plot density plot(xo.den, xlim = c(min(xo), max(xa)), ylim = c(0, max(max(xo.den$y), max(xa.den$y))), main = &quot;&quot;, ylab = expression(f(x))) lines(xa.den, col = &#39;red&#39;) # # Shade upper 5% # xo.vals &lt;- xo.den$x[xo.den$x &gt;= q95] # yo.vals &lt;- xo.den$y[xo.den$x &gt;= q95] # # polygon(c(q95, xo.vals, max(xo.vals)), # c(0, yo.vals, 0), # col = &quot;pink&quot;, # border = NA) } # Add a vertical line at the 95th percentile # abline(v = q95, col = &quot;red&quot;, lty = 2) Sample size This begs the question, how many samples are necessary to make inference of a populations central value? Here lies one of the first critiques of NHST, as we must assume that the overall greater population follows some known distribution, in this case, a normal distribution, and that we are sampling from that population. However, the greater population is actually unknowable outside of simply measuring the entire population. Samples are expensive, so at some point we have to simply accept that we have sampled enough, which is to say done our due diligence to collect an adequate representation of the greater population so we can make inference from it. Below we can show mathematically how many samples are required. The goal of sampling is to create a credible interval which has a given chance of containing \\(u\\), or the population mean. This reasonable level of certainty is typically set at 90-95% due to tradition. However, this runs two risks. 1) 95% levels of confidence means you run a 1 in 20 chance of not capturing the population mean. Equate this to having a 20 round magazine clip loaded into a gun that you are taking on a camping trip to the Little Bighorns in Wyoming for protection against Grizzly bears, in which 1 of the 20 bullets is a blank. Is this a risk you ar willing to take? The second risk is the inverse of the first, which is that this may be an unreasonble and arbritarily high level of confidence that would reduce the percieved validity of aquired data, particularly if the data is novel, but funds or circumstances meant obtaining enough samples to obtain this level of confidence was not possible, and thus the research was not conducted or the findings not reported. The formula is as follows: \\(n = (z_σ/2)^2^ σ^2^ / E^2^\\) and in code for the heifer bodyweight on day 0 Because the heifers range from 376.5125186, 582.4143309, the typical assumption to approximate the population \\(σ\\) is to take 51.4754531, and we might arbritrarily choose our accuracy to be 25 lbs. x = bodyweight$BW0 sigma = diff(range(x))/4 E = 25 n = (1.96^2 * sigma^2)/E^2 n ## [1] 28.8412 So, from this we can interpret that in order to quantify the central limit (\\(u\\)), of the population of heifers within 25 lbs with 95% confidence, we need 28 heifers. Now note how this changes as we tighten our accuracy levels. E = seq(50,1,-1) sigma = diff(range(x))/4 n = c() for (i in E) { n[i] = (1.96^2 * sigma^2)/E[i]^2 } { par(mfrow = c(1,2)) plot(E,n, type = &#39;n&#39;) points(E,n) lines(E,n, col = &#39;blue&#39;, lwd = 2) plot(E,n, xlim = c(5,length(E)), ylim = c(0,500)) lines(E,n, col = &#39;blue&#39;) } So we see that the desired level of accuracy of our measurement is directly related to the number of samples we have to take. The other parameter, which I would remind you we are making using prior assumptions, is the natural variation which exists in the greater population. How does that vary our sample size at a given level of accuracy? E = 25 # accuracy level sigma = seq(1,100,1) n = c() for (i in 1:length(sigma)) { n[i] = (1.96^2 * sigma[i]^2)/E^2 } { plot(sigma,n, type = &#39;n&#39;) points(sigma,n) lines(sigma,n, col = &#39;blue&#39;, lwd = 2) } So here we see that holding accuracy constant and varying the theorized population variance, we dramatically impact the required number of samples we have to take. 3.4 Linear Models Next we embark into linear models, our first cause and effect models. These are extremely powerful statistical tools that are frequently used in science, as they provide information regarding the functional relationship between the response (dependent) and explanatory (independent variables) (Ott and Longnecker 2016). Regression models have a number of applications: Provides a major description of features within the dataset. Provides estimates of the response of values not tested that are intermediary to values tested Provides a mathematical tool to predict the future Shows relationship between cheap to measure variables and expensive to measure variables 3.4.1 Prediction vs. explanation It is important to note the difference between explaining the difference between Prediction vs. Explanation. Prediction makes reference to predicting future values, while explanation focuses on understanding relationships within the current dataset. While the architecture of the models are identical, the thought and process of building them is different. Building explanatory models is easier, as the model is built with the same data it is designed to predict. Prediction is by nature more difficult. It is important to note that the term “prediction” is used in both instances. 3.4.2 Simple Linear Models A simple causal linear model takes the form of \\(y = \\beta_0 + \\beta_1x_1\\), where \\(y\\) represents the dependent variable, \\(\\beta_0\\) is the y intercept of the value of y when x = 0, and \\(\\beta_1\\) is the slope of the line. The assumption of linearity means that we assume the slope of the line doesn’t change as x changes. This is demonstrated below. { fun1 = function(x,b0,b1){y = b0 + x*b1} days = seq(1,45,1) # Days in a feeding trial initialweight = 300 # Initial bodyweight gain = 0.75 # Average Daily Gain, Kgs per day weight = fun1(days, b0 = initialweight, b1 = gain) # Equation for what the animal weighs on each day through the trial plot(days,weight, ylim = c(275,375), main = expression(y == beta[0] + beta[1]*x)) abline(lm(weight~days), col = &#39;blue&#39;) } However, this leaves no room for variation, or stoichasticity, in the relationship. And as we will recall from class, one of the Fundamental principles of statistics is to handle variation in data and causal relationship models. Below is an example of a projected linear growth in BW model with simulated normal varation added into the relationship. fun1 = function(x,b0,b1,e){y = b0 + x*b1 + e} days = seq(1,45,1) # Days in a feeding trial initialweight = 300 # Initial bodyweight gain = 0.75 # Average Daily Gain, Kgs per day variance = rnorm(n = length(days), mean = 0, sd = 10) weight = fun1(x = days,b0 = initialweight, b1 = gain, e = variance) # Equation for what the animal weighs on each day through the trial par(mfrow = c(1,2)) { plot(days,weight, ylim = c(275,375), main = expression(y == beta[0] + beta[1]*x + epsilon)) abline(lm(weight~days), col = &#39;blue&#39;) } { hist(variance) rug(variance, col = &#39;blue&#39;) } The above data is an example of relationships with regularly spaced data collected on the x interval. However, this is not always necessary. Consider the same simulated growth plot below, where the only difference is adding another opportunity for variation along the x axis. fun1 = function(x,b0,b1,e){y = b0 + x*b1 + e} days = round(runif(100, min = 0,max = 45)) hist(days) initialweight = 300 # Initial bodyweight gain = 0.75 # Average Daily Gain, Kgs per day variance = rnorm(n = length(days), mean = 0, sd = 10) weight = fun1(x = days,b0 = initialweight, b1 = gain, e = variance) # Equation for what the animal weighs on each day through the trial par(mfrow = c(1,2)) { plot(days,weight, ylim = c(275,375), main = expression(y == beta[0] + beta[1]*x + epsilon)) abline(lm(weight~days), col = &#39;blue&#39;) } { hist(variance) rug(variance, col = &#39;blue&#39;) } This simulation represents the type of data we might see when using precision livestock technology such as smartscales. We have randomly created a sequence of days ranging from day 1 to day 45, where the animal may visit between 1 to an infinite number of times. Note how if you copy and paste this code, that the number of visits per day, the length of bodyweights recorded, changes each time you run it. Yet, we are still able to fit the same linear regression model? This is perfectly acceptable. Next lets use base R functions to fit a linear regression model to the data. The software will identify the line of best fit by minimizing the prediction error of the residuals, the distance between each observed data point and the regression line, using a method called the least squares prediction method. weight.lm = lm(weight~days) # Create linear regression model and assign it to object weight.lm weight.lm ## ## Call: ## lm(formula = weight ~ days) ## ## Coefficients: ## (Intercept) days ## 297.9039 0.8194 summary(weight.lm) ## ## Call: ## lm(formula = weight ~ days) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.397 -6.246 -0.173 7.296 20.956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 297.90390 1.70490 174.73 &lt;2e-16 *** ## days 0.81945 0.06761 12.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.678 on 98 degrees of freedom ## Multiple R-squared: 0.5998, Adjusted R-squared: 0.5957 ## F-statistic: 146.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(weight.lm) Because the simulated data was constructed to satisfy all the assumptions of simple linear models, i.e. linearity, normality of the residuals etc, the resulting plots from the simple linear model are ideal. But what would they look like if we introduce non-normal variance to the residuals? Lets plot it and find out. fun1 = function(x,b0,b1,e){y = b0 + x*b1 + e} days = round(runif(100, min = 0,max = 45)) hist(days) initialweight = 300 # Initial bodyweight gain = 0.75 # Average Daily Gain, Kgs per day variance = rnorm(n = length(days), mean = 0, sd = 10) # normal variance variance = exp(variance) - mean(exp(variance)) # apply a skewing scalar to the variance variance = variance/sd(variance) * 10 # Standardize and multiply to achieve desired SD weight = fun1(x = days,b0 = initialweight, b1 = gain, e = variance) # Equation for what the animal weighs on each day through the trial par(mfrow = c(1,2)) { plot(days,weight, main = expression(y == beta[0] + beta[1]*x + epsilon)) abline(lm(weight~days), col = &#39;blue&#39;) hist(variance) rug(variance, col = &#39;blue&#39;) } weight.lm = lm(weight~days) # Create linear regression model and assign it to object weight.lm weight.lm ## ## Call: ## lm(formula = weight ~ days) ## ## Coefficients: ## (Intercept) days ## 300.136 0.744 summary(weight.lm) ## ## Call: ## lm(formula = weight ~ days) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.304 -1.226 -1.151 -1.088 98.307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 300.13618 1.96989 152.362 &lt;2e-16 *** ## days 0.74401 0.07457 9.977 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.05 on 98 degrees of freedom ## Multiple R-squared: 0.5039, Adjusted R-squared: 0.4989 ## F-statistic: 99.55 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(weight.lm) Here I have used the exponential of the variation to create a residual structure artificaially skewed to the right. Note the changes in the QQ plots and the structure of the residuals. Example Simple Linear Model Finally, lets apply this to our toy datasets. # Libraries ---- library(tidyverse) # Graphing ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(data.table) # Reading data and manipulating data ## ## Attaching package: &#39;data.table&#39; ## ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose library(readxl) # Reading excel library(qthink) # Data data(&quot;bodyweight&quot;) # head(bodyweight) d.bw = melt.data.table(data = bodyweight, measure.vars = c(5:16), value.name = &#39;BW&#39;, variable.name = &#39;name&#39;) d.bw$Day = parse_number(as.character(d.bw$name)) # Data wrangling days = seq(min(d.bw$Day), max(d.bw$Day), by = 1)### Assign day days = days[days &gt;= 0] days ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## [26] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 ## [51] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 d.days = data.table(Day = days) VIDs = data.table(VID = unique(d.bw$VID)) # Get list of unique animals d.daysvid = merge(days, VIDs, by = NULL) %&gt;% # Create a table with one animal per day observation as.data.table names(d.daysvid) = c(&#39;Day&#39;,&#39;VID&#39;) # Assign names to the table names(d.days) ## [1] &quot;Day&quot; d.bw2 = merge.data.table(d.daysvid, d.bw, by = c(&#39;VID&#39;,&#39;Day&#39;), all = T) # merge bodyweight with our days vid table. # Run Linear Regression model lm.bw = lm(BW ~ Day, data = d.bw2) summary(lm.bw) ## ## Call: ## lm(formula = BW ~ Day, data = d.bw2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -179.964 -34.337 0.374 36.372 216.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 477.28860 2.43741 195.82 &lt;2e-16 *** ## Day 1.63822 0.06541 25.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 58.27 on 934 degrees of freedom ## (4758 observations deleted due to missingness) ## Multiple R-squared: 0.4017, Adjusted R-squared: 0.4011 ## F-statistic: 627.2 on 1 and 934 DF, p-value: &lt; 2.2e-16 3.4.3 Simple Anovas At their core, Analysis of Variance Analysises are linear models which use categorical independent variables. Some books teach ANOVAs separately from linear models, preferring instead to refer to them as “describing differences amongst populations” (Ott and Longnecker 2016). However, I do not find this method useful as ANOVAs are ran automatically when the lm function is called with categorical independent variables. Thus, I am going to include them here in the discussion on linear models, with the understanding that the math does change somewhat. This also allows me the opportunity to demonstrate that it is sometimes possible to run independent variables as either categorical or continuous, and doing so can be a product component of the exploratory data process. Here I simulate a series of datum demonstrative of data which may be analyzed using an ANOVA. # Create a data table with multiple variables d = data.table(t1 = rnorm(100, mean = 150, sd = 50), t2 = rnorm(100, mean = 200, sd = 30), t3 = rnorm(100, mean = 250, sd = 75)) d2 = melt.data.table(data = d, measure.vars = c(1:3)) # Melt the data into long format d2 %&gt;% ggplot(aes(x = variable, y = value, fill = variable))+ geom_boxplot() aov.mod = aov(value ~ variable, data = d2) summary(aov.mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variable 2 443650 221825 63.98 &lt;2e-16 *** ## Residuals 297 1029792 3467 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 coef(aov.mod) ## (Intercept) variablet2 variablet3 ## 151.81016 48.28339 94.18654 plot(aov.mod) Just to prove my point, look at the values collected from the ANOVA analysis ran above, and compare to the lm model ran below. lm.mod = lm(value ~ variable, data = d2) summary(lm.mod) ## ## Call: ## lm(formula = value ~ variable, data = d2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -220.768 -31.893 3.146 36.768 172.895 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 151.810 5.888 25.781 &lt; 2e-16 *** ## variablet2 48.283 8.327 5.798 1.71e-08 *** ## variablet3 94.187 8.327 11.310 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 58.88 on 297 degrees of freedom ## Multiple R-squared: 0.3011, Adjusted R-squared: 0.2964 ## F-statistic: 63.98 on 2 and 297 DF, p-value: &lt; 2.2e-16 anova(lm.mod) ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variable 2 443650 221825 63.976 &lt; 2.2e-16 *** ## Residuals 297 1029792 3467 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What do you notice about the results from the outputs of both methods? How do the central population estimates compare to the estimates we simulated in our datasets? What can we conclude about the differences in our population level (categorical or treatment) means? The most important statistic in an ANOVA table is the **F Statistic. The F statistic informs us if the within sample* variation is greater than the between sample variation. if the variability among the sample means is large compared to the within sample means, we are more apt to conclude that there is a difference. Alternatively, if the between sample variation is small compared to the within sample varation, we are less likely to conclude that there is a difference. Use the code above to create an example with and without a significant F value. Example ANOVA Lets use the above body-weight data to demonstrate how we might use an ANOVA to analys the same data in a categorical format that we just analysed in a linear model format by changing the type of data in the “day” column to characher. d.bw$Day = as.character(d.bw$Day) d.bw %&gt;% ggplot(aes(x=Day, y = BW, fill = Day))+ geom_boxplot() aov.bw = aov(BW ~ Day, data = d.bw) summary(aov.bw) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Day 11 2344205 213110 66.59 &lt;2e-16 *** ## Residuals 924 2957213 3200 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 coef(aov.bw) ## (Intercept) Day-42 Day0 Day14 Day21 Day28 ## 495.57692 -66.26923 -20.28205 -16.67949 12.01282 11.65385 ## Day35 Day42 Day49 Day56 Day7 Day70 ## 36.87179 46.88462 61.73077 83.78205 -30.70513 118.60256 plot(aov.bw) How do these results look compared to the results from the linear model? How are they similar? How are they different? What assumption is being made about the independent variable here that is not being accounted for that is be accounted for in the linear regression model above using time as a continuous value? 3.5 Demonstrating Equivalence I do have some concerns with the message that is conveyed by the title and approach taken in the statistical methods. Specifically, remember the definition of a p-value is “the probability of obtaining a value of the test statistic that is as likely or more likely to reject the null hypothesis (H0) as the actual observed value of the test statistic, assuming the null hypothesis is true.” The null hypothesis tested by a general linear mixed effects model for your study is generally “Newly weaned calves limit fed a high concentrate diet perform does not alter growth, performance, and efficiency compared to calves fed an ad libitum forage based diet”, with H0: ALF = LFC and HA: ALF ≠ LFC. A high p-value in this instance indicates a failure to reject the null hypothesis. No matter how large the p-value may be, it does not provide support for the null hypothesis. Failure to reject the null hypothesis means one must now become concerned with the power, or n, of the study, and then move forward to show sufficient effort was made to demonstrate equivalence, or conversely find differences, between treatments. This is typically illustrated by determining an acceptable delta, (difference between treatment) and then conducting a power analysis to show how many animals would be required to obtain a significant p-value at that acceptable delta given the variation in the system. If this is unnecessarily restrictive or feels difficult to communicate, more studies are moving towards a more Bayesian type of approach, which allows you to specify the prior distribution of the effect size (informed by what is biologically or economically relevant) and then calculate the likelihood of observing no meaningful effect. 3.6 Liklihood vs. Probability (inspired by the following medium article) References Fisher, R. A. 1932. “Inverse Probability and the Use of Likelihood.” Mathematical Proceedings of the Cambridge Philosophical Society 28 (3): 257–61. https://doi.org/10.1017/S0305004100010094. ———. 1960. The Design of Experiments. 6th ed. London; Edinburgh: Oliver; Boyd. Hagen, Richard L. n.d. “In Praise of the Null Hypothesis Statistical Test.” Ioannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Mayo, Deborah G. 2018. Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. New York: Cambridge University Press. Ott, R. Lymann, and Michael Longnecker. 2016. Statistical Methods and Data Analysis. 2nd ed. Boston, MA: Cengage Learning. Perezgonzalez, Jose D. 2015. “Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing.” Frontiers in Psychology 6 (March). https://doi.org/10.3389/fpsyg.2015.00223. Sedgwick, Philip M. n.d. “Trials and Tribulations of Teaching NHST in the Health Sciences.” Stephens, Philip A., Steven W. Buskirk, and Carlos Martínez Del Rio. 2007. “Inference in Ecology and Evolution.” Trends in Ecology &amp; Evolution 22 (4): 192–97. https://doi.org/10.1016/j.tree.2006.12.003. Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p -Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108. Wu, Jinshan. 2018. “Is There an Intrinsic Logical Error in Null Hypothesis Significance Tests? Commentary on: ‘Null Hypothesis Significance Tests. A Mix-up of Two Different Theories: The Basis for Widespread Confusion and Numerous Misinterpretations’.” Scientometrics 115 (1): 621–25. https://doi.org/10.1007/s11192-018-2656-3. "],["experimental-design.html", "Chapter 4 Experimental Design 4.1 Why is sound experimental design important? 4.2 Completely Randomized 4.3 Randomized Complete Block Design 4.4 Latin Squares 4.5 Split plot design 4.6 Strip Plot design", " Chapter 4 Experimental Design 4.1 Why is sound experimental design important? Experimental design involves steps taken at the beginning of a study to control for variation and protect the validity of statistical results. Good experimental design paired with proper statistical methods ensures robust findings while minimizing animal suffering and resource expenditure (festingReductionAnimalUse1994?; johnsonPracticalAspectsExperimental2002?; lehnerDESIGNEXECUTIONANIMAL?). Experiments deal with field variability and animal diversity. Designs seek to control the variation to allow for treatment effects to show themselves in a repeatable manner. 4.2 Completely Randomized Completely randomized designs are used when comparing more than one treatment. Each experimental unit is assumed to be a random selection from the population. # Set seed for reproducibility set.seed(123) # Define parameters n_per_treatment = 10 # number of replicates per treatment treatments = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) n_total = n_per_treatment * length(treatments) # Create treatment vector (randomized) treatment = sample(rep(treatments, each = n_per_treatment)) # Simulate response variable (e.g., plant height) # Assume different means for each treatment response = rnorm(n_total, mean = ifelse(treatment == &quot;A&quot;, 20, ifelse(treatment == &quot;B&quot;, 25, 30)), sd = 3) # Create data frame crd_data = data.frame( PlantID = 1:n_total, Treatment = treatment, Height = response ) # View first few rows head(crd_data) ## PlantID Treatment Height ## 1 1 B 21.74290 ## 2 2 B 24.74373 ## 3 3 B 28.21183 ## 4 4 A 19.56382 ## 5 5 A 16.50337 ## 6 6 B 22.54445 # Summary statistics aggregate(Height ~ Treatment, data = crd_data, mean) ## Treatment Height ## 1 A 19.12492 ## 2 B 23.89719 ## 3 C 30.02318 # ANOVA anova_result &lt;- aov(Height ~ Treatment, data = crd_data) summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 596.9 298.46 32.55 6.4e-08 *** ## Residuals 27 247.6 9.17 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Basic scatter plot ggplot(crd_data, aes(x = Treatment, y = Height)) + geom_jitter(width = 0.2, height = 0, color = &quot;steelblue&quot;, size = 2) + labs(title = &quot;Plant Height by Fertilizer Treatment&quot;, x = &quot;Treatment&quot;, y = &quot;Height (cm)&quot;) + theme_minimal() # Boxplot boxplot(Height ~ Treatment, data = crd_data, main = &quot;Plant Height by Fertilizer Treatment&quot;, xlab = &quot;Treatment&quot;, ylab = &quot;Height (cm)&quot;, col = c(&quot;lightblue&quot;, &quot;lightgreen&quot;, &quot;lightpink&quot;)) # Assume crd_data is already created from previous simulation # If not, run the CRD simulation code first 4.3 Randomized Complete Block Design The randomized complete block is useful for blocking across some known source of variation. It allows for more precise comparison between treatments when their is known varation within the experimental units. It has the added advantage of providing a comparison across the blocks if desired. # Randomized Complete Block ----- # Load libraries library(dplyr) library(ggplot2) # Define factors treatments &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) blocks &lt;- paste0(&quot;Field&quot;, 1:5) # Create RCBD layout set.seed(42) # Define factors treatments &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) blocks &lt;- paste0(&quot;Block&quot;, 1:5) # Create RCBD layout rcbd &lt;- expand.grid(Block = blocks, Treatment = treatments) %&gt;% group_by(Block) %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% ungroup() %&gt;% mutate( # Simulate subplot-level variation within each block SubplotNoise = rnorm(n(), mean = 0, sd = 4), # Increased SD for within-field variation # Simulate yield: treatment + block + subplot noise + residual error Yield = 50 + ifelse(Treatment == &quot;A&quot;, 5, ifelse(Treatment == &quot;B&quot;, 10, ifelse(Treatment == &quot;C&quot;, 15, 20))) + as.numeric(gsub(&quot;Block&quot;, &quot;&quot;, Block)) * 3 + SubplotNoise + rnorm(n(), mean = 0, sd = 2) ) # Add numeric position for plotting rcbd &lt;- rcbd %&gt;% group_by(Block) %&gt;% mutate(Plot = row_number()) %&gt;% ungroup() # Plot layout ggplot(rcbd, aes(x = Plot, y = Block, fill = Treatment)) + geom_tile(color = &quot;black&quot;) + geom_text(aes(label = Treatment), size = 5) + # scale_y_reverse(breaks = rcbd$Block, labels = rcbd$Block) + scale_fill_brewer(palette = &quot;Set2&quot;) + labs(title = &quot;RCBD Treatment Layout&quot;, x = &quot;Plot within Block&quot;, y = &quot;Block&quot;) + theme_minimal() head(rcbd) ## # A tibble: 6 × 5 ## Block Treatment SubplotNoise Yield Plot ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Block1 A -4.34 57.9 1 ## 2 Block2 B 6.45 72.8 1 ## 3 Block3 D 0.143 77.4 1 ## 4 Block4 D 5.26 89.2 1 ## 5 Block5 D 3.91 87.5 1 ## 6 Block1 D 3.53 74.4 2 ggplot(rcbd, aes(x = Treatment, y = Yield, fill = Block)) + geom_boxplot() + theme_minimal() + labs(title = &quot;RCBD: Treatment Effects Across Blocks&quot;) ggplot(rcbd, aes(x = Treatment, y = Yield, color = Block)) + geom_jitter(width = 0.2, height = 0, size = 2) + theme_minimal() + labs(title = &quot;RCBD with Increased Within-Block Variation&quot;) # Fit ANOVA model model_rcbd &lt;- aov(Yield ~ Treatment + Block, data = rcbd) summary(model_rcbd) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 3 780.3 260.09 14.911 0.000238 *** ## Block 4 497.1 124.28 7.125 0.003534 ** ## Residuals 12 209.3 17.44 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack model_mixed &lt;- lmer(Yield ~ Treatment + (1 | Block), data = rcbd) summary(model_mixed) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ Treatment + (1 | Block) ## Data: rcbd ## ## REML criterion at convergence: 105.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4188 -0.4879 0.1124 0.4139 1.6189 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Block (Intercept) 26.71 5.168 ## Residual 17.44 4.177 ## Number of obs: 20, groups: Block, 5 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 65.366 2.972 21.997 ## TreatmentB 3.024 2.641 1.145 ## TreatmentC 10.595 2.641 4.011 ## TreatmentD 15.882 2.641 6.012 ## ## Correlation of Fixed Effects: ## (Intr) TrtmnB TrtmnC ## TreatmentB -0.444 ## TreatmentC -0.444 0.500 ## TreatmentD -0.444 0.500 0.500 4.4 Latin Squares The Latin Square is an extension of the randomized complete block design, but rather than the treatments being randomly assigned within each block, they are strategically allocated such that each treatment occurs once between each row and each column. # Define factors treatments &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) latin_square &lt;- matrix(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), nrow = 4, byrow = TRUE) # Create data frame df &lt;- expand.grid(Row = factor(1:4), Column = factor(1:4)) %&gt;% mutate(Treatment = as.vector(t(latin_square)), # Simulate yield with treatment effect + row/column noise Yield = 50 + ifelse(Treatment == &quot;A&quot;, 5, ifelse(Treatment == &quot;B&quot;, 10, ifelse(Treatment == &quot;C&quot;, 15, 20))) + as.numeric(Row)*2 + as.numeric(Column)*1.5 + rnorm(16, mean = 0, sd = 2)) head(df) ## Row Column Treatment Yield ## 1 1 1 A 57.93586 ## 2 2 1 B 69.06357 ## 3 3 1 C 73.74532 ## 4 4 1 D 73.82487 ## 5 1 2 B 66.96279 ## 6 2 2 C 71.76247 ggplot(df, aes(x = Column, y = Row, fill = Treatment)) + geom_tile(color = &quot;black&quot;) + geom_text(aes(label = Treatment), size = 5) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + labs(title = &quot;Latin Square Design Layout&quot;) # Fit ANOVA model model &lt;- aov(Yield ~ Row + Column + Treatment, data = df) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Row 3 27.8 9.25 1.698 0.265817 ## Column 3 102.4 34.15 6.264 0.028037 * ## Treatment 3 473.9 157.96 28.980 0.000574 *** ## Residuals 6 32.7 5.45 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.5 Split plot design library(dplyr) library(ggplot2) library(lme4) # Define factors blocks &lt;- paste0(&quot;Field&quot;, 1:4) irrigation &lt;- c(&quot;Low&quot;, &quot;High&quot;) fertilizer &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # Create layout set.seed(42) splitplot &lt;- expand.grid(Block = blocks, Irrigation = irrigation, Fertilizer = fertilizer) %&gt;% mutate( # Simulate effects IrrigationEff = ifelse(Irrigation == &quot;High&quot;, 10, 0), FertilizerEff = case_when( Fertilizer == &quot;A&quot; ~ 5, Fertilizer == &quot;B&quot; ~ 10, Fertilizer == &quot;C&quot; ~ 15 ), BlockEff = as.numeric(gsub(&quot;Field&quot;, &quot;&quot;, Block)) * 2, Residual = rnorm(n(), 0, 3), Yield = 50 + IrrigationEff + FertilizerEff + BlockEff + Residual ) # Add plot position for layout splitplot &lt;- splitplot %&gt;% group_by(Block, Irrigation) %&gt;% mutate(Plot = row_number()) %&gt;% ungroup() ggplot(splitplot, aes(x = Plot, y = Block, fill = Fertilizer)) + geom_tile(color = &quot;black&quot;) + facet_wrap(~ Irrigation) + geom_text(aes(label = Fertilizer), size = 5) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + labs(title = &quot;Split-Plot Layout: Irrigation × Fertilizer&quot;, x = &quot;Subplot within Irrigation&quot;, y = &quot;Block&quot;) # Mixed model: Irrigation as main plot, Fertilizer as subplot model_split &lt;- lmer(Yield ~ Irrigation * Fertilizer + (1 | Block/Irrigation), data = splitplot) summary(model_split) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ Irrigation * Fertilizer + (1 | Block/Irrigation) ## Data: splitplot ## ## REML criterion at convergence: 103.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.37592 -0.43900 0.08326 0.29437 1.68979 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Irrigation:Block (Intercept) 2.374 1.541 ## Block (Intercept) 16.151 4.019 ## Residual 6.420 2.534 ## Number of obs: 24, groups: Irrigation:Block, 8; Block, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 61.352 2.497 24.568 ## IrrigationHigh 9.935 2.097 4.738 ## FertilizerB 7.809 1.792 4.358 ## FertilizerC 5.603 1.792 3.127 ## IrrigationHigh:FertilizerB -4.969 2.534 -1.961 ## IrrigationHigh:FertilizerC 2.327 2.534 0.919 ## ## Correlation of Fixed Effects: ## (Intr) IrrgtH FrtlzB FrtlzC IrH:FB ## IrrigatnHgh -0.420 ## FertilizerB -0.359 0.427 ## FertilizerC -0.359 0.427 0.500 ## IrrgtnHg:FB 0.254 -0.604 -0.707 -0.354 ## IrrgtnHg:FC 0.254 -0.604 -0.354 -0.707 0.500 4.6 Strip Plot design # Define factors blocks &lt;- paste0(&quot;Field&quot;, 1:3) tillage &lt;- c(&quot;Conventional&quot;, &quot;No-Till&quot;) fertilizer &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # Create layout set.seed(42) stripplot &lt;- expand.grid(Block = blocks, Tillage = tillage, Fertilizer = fertilizer) %&gt;% mutate( TillageEff = ifelse(Tillage == &quot;No-Till&quot;, 8, 0), FertilizerEff = case_when( Fertilizer == &quot;A&quot; ~ 5, Fertilizer == &quot;B&quot; ~ 10, Fertilizer == &quot;C&quot; ~ 15 ), BlockEff = as.numeric(gsub(&quot;Field&quot;, &quot;&quot;, Block)) * 3, Residual = rnorm(n(), 0, 3), Yield = 50 + TillageEff + FertilizerEff + BlockEff + Residual ) # Add plot position for layout stripplot &lt;- stripplot %&gt;% group_by(Block, Tillage) %&gt;% mutate(Plot = row_number()) %&gt;% ungroup() ggplot(stripplot, aes(x = Fertilizer, y = Tillage, fill = Fertilizer)) + geom_tile(color = &quot;black&quot;) + facet_wrap(~ Block) + geom_text(aes(label = round(Yield, 1)), size = 4) + scale_fill_brewer(palette = &quot;Set2&quot;) + theme_minimal() + labs(title = &quot;Strip Plot Layout: Tillage × Fertilizer&quot;, x = &quot;Vertical Strip (Fertilizer)&quot;, y = &quot;Horizontal Strip (Tillage)&quot;) # Mixed model: Tillage and Fertilizer as fixed, Block as random model_strip &lt;- lmer(Yield ~ Tillage * Fertilizer + (1 | Block), data = stripplot) summary(model_strip) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ Tillage * Fertilizer + (1 | Block) ## Data: stripplot ## ## REML criterion at convergence: 73.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4053 -0.5565 0.1948 0.5293 1.1583 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Block (Intercept) 5.865 2.422 ## Residual 12.314 3.509 ## Number of obs: 18, groups: Block, 3 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 62.1694 2.4617 25.255 ## TillageNo-Till 7.7616 2.8652 2.709 ## FertilizerB 7.2659 2.8652 2.536 ## FertilizerC 7.0296 2.8652 2.453 ## TillageNo-Till:FertilizerB 0.3319 4.0520 0.082 ## TillageNo-Till:FertilizerC -0.2654 4.0520 -0.065 ## ## Correlation of Fixed Effects: ## (Intr) TllN-T FrtlzB FrtlzC TN-T:FB ## TillagN-Tll -0.582 ## FertilizerB -0.582 0.500 ## FertilizerC -0.582 0.500 0.500 ## TllgN-Tl:FB 0.412 -0.707 -0.707 -0.354 ## TllgN-Tl:FC 0.412 -0.707 -0.354 -0.707 0.500 Fisher, R. A. 1932. “Inverse Probability and the Use of Likelihood.” Mathematical Proceedings of the Cambridge Philosophical Society 28 (3): 257–61. https://doi.org/10.1017/S0305004100010094. ———. 1960. The Design of Experiments. 6th ed. London; Edinburgh: Oliver; Boyd. Hagen, Richard L. n.d. “In Praise of the Null Hypothesis Statistical Test.” Ioannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Mayo, Deborah G. 2018. Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. New York: Cambridge University Press. Ott, R. Lymann, and Michael Longnecker. 2016. Statistical Methods and Data Analysis. 2nd ed. Boston, MA: Cengage Learning. Perezgonzalez, Jose D. 2015. “Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing.” Frontiers in Psychology 6 (March). https://doi.org/10.3389/fpsyg.2015.00223. Sedgwick, Philip M. n.d. “Trials and Tribulations of Teaching NHST in the Health Sciences.” Stephens, Philip A., Steven W. Buskirk, and Carlos Martínez Del Rio. 2007. “Inference in Ecology and Evolution.” Trends in Ecology &amp; Evolution 22 (4): 192–97. https://doi.org/10.1016/j.tree.2006.12.003. Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p -Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108. Wu, Jinshan. 2018. “Is There an Intrinsic Logical Error in Null Hypothesis Significance Tests? Commentary on: ‘Null Hypothesis Significance Tests. A Mix-up of Two Different Theories: The Basis for Widespread Confusion and Numerous Misinterpretations’.” Scientometrics 115 (1): 621–25. https://doi.org/10.1007/s11192-018-2656-3. "]]
